{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Latent Dirichlet Allocation - An End-to-End  Example\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Training](#Training)\n",
    "1. [Inference](#Inference)\n",
    "1. [Epilogue](#Epilogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "\n",
    "Amazon SageMaker LDA is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. Latent Dirichlet Allocation (LDA) is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics.\n",
    "\n",
    "In this notebook we will use the Amazon SageMaker LDA algorithm to train an LDA model on some example synthetic data. We will then use this model to classify (perform inference on) the data. The main goals of this notebook are to,\n",
    "\n",
    "* learn how to obtain and store data for use in Amazon SageMaker,\n",
    "* create an AWS SageMaker training job on a data set to produce an LDA model,\n",
    "* use the LDA model to perform inference with an Amazon SageMaker endpoint.\n",
    "\n",
    "The following are ***not*** goals of this notebook:\n",
    "\n",
    "* understand the LDA model,\n",
    "* understand how the Amazon SageMaker LDA algorithm works,\n",
    "* interpret the meaning of the inference output\n",
    "\n",
    "If you would like to know more about these things take a minute to run this notebook and then check out the SageMaker LDA Documentation and the **LDA - Science.ipynb** notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import io, json, os, re, sys, time\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# some helpful utility functions are defined in the Python module\n",
    "# \"generate_example_data\" located in the same directory as this\n",
    "# notebook\n",
    "from generate_example_data import generate_griffiths_data, plot_lda, match_estimated_topics\n",
    "\n",
    "# accessing SageMaker via Python\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "from sagemaker.amazon.common import write_numpy_to_dense_tensor\n",
    "from mxnet.recordio import MXRecordIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "***\n",
    "\n",
    "*This notebook was created and tested on an ml.m4.xlarge notebook instance.*\n",
    "\n",
    "Before we do anything at all, we need data! We also need to setup our AWS credentials so that AWS SageMaker can store and access data. In this section we will do four things:\n",
    "\n",
    "1. [Setup AWS Credentials](#SetupAWSCredentials)\n",
    "1. [Obtain Example Dataset](#ObtainExampleDataset)\n",
    "1. [Inspect Example Data](#InspectExampleData)\n",
    "1. [Store Data on S3](#StoreDataonS3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup AWS Credentials\n",
    "\n",
    "We first need to specify some AWS credentials; specifically data locations and access roles. This is the only cell of this notebook that you will need to edit. In particular, we need the following data:\n",
    "\n",
    "* `bucket` - An S3 bucket accessible by this account.\n",
    "  * Used to store input training data and model data output.\n",
    "  * Should be withing the same region as this notebook instance, training, and hosting.\n",
    "* `prefix` - The location in the bucket where this notebook's input and and output data will be stored. (The default value is sufficient.)\n",
    "* `role` - The IAM Role ARN used to give training and hosting access to your data.\n",
    "  * See documentation on how to create these.\n",
    "  * The script below will try to determine an appropriate Role ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "bucket = '<your_s3_bucket_name_here>'\n",
    "prefix = 'sagemaker/lda_rosetta_stone'\n",
    "\n",
    "assumed_role = boto3.client('sts').get_caller_identity()['Arn']\n",
    "role = re.sub(r'^(.+)sts::(\\d+):assumed-role/(.+?)/.*$', r'\\1iam::\\2:role/\\3', assumed_role)\n",
    "\n",
    "\n",
    "print('Training input/output will be stored in {}/{}'.format(bucket, prefix))\n",
    "print('\\nIAM Role: {}'.format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Example Data\n",
    "\n",
    "\n",
    "We generate some example synthetic document data. For the purposes of this notebook we will omit the details of this process. All we need to know is that each piece of data, commonly called a *\"document\"*, is a vector of integers representing *\"word counts\"* within the document. In this particular example there are a total of 25 words in the *\"vocabulary\"*.\n",
    "\n",
    "$$\n",
    "\\underbrace{w}_{\\text{document}} = \\overbrace{\\big[ w_1, w_2, \\ldots, w_V \\big] }^{\\text{word counts}},\n",
    "\\quad\n",
    "V = \\text{vocabulary size}\n",
    "$$\n",
    "\n",
    "These data are based on that used by Griffiths and Steyvers in their paper [Finding scietific topics](http://psiexp.ss.uci.edu/research/papers/sciencetopics.pdf). For more information, see the **LDA - Science.ipynb** notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating example data...')\n",
    "num_documents = 6000\n",
    "known_alpha, known_beta, documents, topic_mixtures = generate_griffiths_data(\n",
    "    num_documents=num_documents, num_topics=5)\n",
    "num_topics, vocabulary_size = known_beta.shape\n",
    "\n",
    "\n",
    "# separate the generated data into training and tests subsets\n",
    "num_documents_training = int(0.9*num_documents)\n",
    "num_documents_test = num_documents - num_documents_training\n",
    "\n",
    "documents_training = documents[:num_documents_training]\n",
    "documents_test = documents[num_documents_training:]\n",
    "\n",
    "topic_mixtures_training = topic_mixtures[:num_documents_training]\n",
    "topic_mixtures_test = topic_mixtures[num_documents_training:]\n",
    "\n",
    "print('documents_training.shape = {}'.format(documents_training.shape))\n",
    "print('documents_test.shape = {}'.format(documents_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Example Data\n",
    "\n",
    "*What does the example data actually look like?* Below we print an example document as well as its corresponding known *topic-mixture*. A topic-mixture serves as the \"label\" in the LDA model. It describes the ratio of topics from which the words in the document are found.\n",
    "\n",
    "For example, if the topic mixture of an input document $\\mathbf{w}$ is,\n",
    "\n",
    "$$\\theta = \\left[ 0.3, 0.2, 0, 0.5, 0 \\right]$$\n",
    "\n",
    "then $\\mathbf{w}$ is 30% generated from the first topic, 20% from the second topic, and 50% from the fourth topic. For more information see **How LDA Works** in the documentation as well as the **LDA - Science.ipynb** notebook.\n",
    "\n",
    "Below, we compute the topic mixtures for the first few traning documents. As we can see, each document is a vector of word counts from the 25-word vocabulary and its topic-mixture is a probability distribution across the 10 topics used to generate the sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First training document =\\n{}'.format(documents[0]))\n",
    "print('\\nVocabulary size = {}'.format(vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Known topic mixture of first document =\\n{}'.format(topic_mixtures_training[0]))\n",
    "print('\\nNumber of topics = {}'.format(num_topics))\n",
    "print('Sum of elements = {}'.format(topic_mixtures_training[0].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, when we perform inference on the training data set we will compare the inferred topic mixture to this known one.\n",
    "\n",
    "\n",
    "Human beings are visual creatures, so it might be helpful to come up with a visual representation of these documents. In the below plots, each pixel of a document represents a word. The greyscale intensity is a measure of how frequently that word occurs. Below we plot the first few documents of the training set reshaped into 5x5 pixel grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_lda(documents_training, nrows=3, ncols=4, cmap='gray_r', with_colorbar=True)\n",
    "fig.suptitle('Example Document Word Counts')\n",
    "fig.set_dpi(160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data on S3\n",
    "\n",
    "A SageMaker training job needs access to training data stored in an S3 bucket. Although training can accept data of various formats we convert the documents MXNet RecordIO Protobuf format before uploading to the S3 bucket defined at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents_training to Protobuf RecordIO format\n",
    "fname = 'data.pbr'\n",
    "recordio = MXRecordIO(fname, 'w')\n",
    "write_numpy_to_dense_tensor(recordio, documents_training)\n",
    "recordio.close()\n",
    "\n",
    "# upload to S3 in bucket/prefix/train\n",
    "s3_object = os.path.join(prefix, 'train', fname)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_object).upload_file(fname)\n",
    "\n",
    "print('Uploaded data to S3: {}/{}'.format(bucket, s3_object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "***\n",
    "\n",
    "Once the data is preprocessed and available in a recommended format the next step is to train our model on the data. There are number of parameters required by SageMaker LDA configurng the model and defining the computational environment in which training will take place.\n",
    "\n",
    "Particular to a SageMaker LDA training job are the following hyperparameters:\n",
    "\n",
    "* **`num_topics`** - The number of topics or categories in the LDA model.\n",
    "  * Usually, this is not known a priori.\n",
    "  * However, in this example we know that the data is generated by five topics.\n",
    "\n",
    "* **`feature_dim`** - The size of the *\"vocabulary\"*, in LDA parlance.\n",
    "  * In this example, this is equal 25.\n",
    "\n",
    "* **`mini_batch_size`** - The number of input training documents.\n",
    "\n",
    "* **`alpha0`** - *(optional)* a measurement of how \"mixed\" are the topic-mixtures.\n",
    "  * When `alpha0` is small the data tends to be represented by one or few topics.\n",
    "  * When `alpha0` is large the data tends to be an even combination of several or many topics.\n",
    "\n",
    "In addition to these LDA model hyperparameters, we provide additional parameters defining things like the EC2 instance type on which training will run, the S3 bucket containing the data, and the AWS access role. Note that,\n",
    "\n",
    "* Recommended instance type: `ml.c4`\n",
    "* Current limitations:\n",
    "  * SageMaker LDA *training* can only run on a single instance.\n",
    "  * SageMaker LDA does not take advantage of GPU hardware.\n",
    "  * (The Amazon AI Algorithms team is working hard to provide these capabilities in a future release!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {\n",
    "    'us-west-2': '266724342769.dkr.ecr.us-west-2.amazonaws.com/lda:latest',\n",
    "    'us-east-1': '766337827248.dkr.ecr.us-east-1.amazonaws.com/lda:latest',\n",
    "    'us-east-2': '999911452149.dkr.ecr.us-east-2.amazonaws.com/lda:latest',\n",
    "    'eu-west-1': '999678624901.dkr.ecr.eu-west-1.amazonaws.com/lda:latest'\n",
    "}\n",
    "region_name = boto3.Session().region_name\n",
    "container = containers[region_name]\n",
    "\n",
    "print('Using SageMaker LDA container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a name for this training job. to better distinguish this training job\n",
    "# from others we append a timestamp.\n",
    "job_name_prefix = 'lda-rosetta-stone-notebook'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "\n",
    "# set up the parameters for the SageMaker create_raining_job call. This includes\n",
    "# things like\n",
    "#   * which algorithm image to use\n",
    "#   * algorithm hyperparameters\n",
    "#   * S3 locations of input/output data\n",
    "training_params = {\n",
    "    'AlgorithmSpecification': {\n",
    "        'TrainingImage': container,\n",
    "        'TrainingInputMode': 'File',\n",
    "    },\n",
    "    'HyperParameters': {\n",
    "        'num_topics': str(num_topics),\n",
    "        'feature_dim': str(vocabulary_size),\n",
    "        'mini_batch_size': str(num_documents_training),\n",
    "        'alpha0': str(1.0),\n",
    "    },\n",
    "    'InputDataConfig': [\n",
    "        {\n",
    "            'ChannelName': 'train',\n",
    "            'CompressionType': 'None',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': 's3://{}/{}/training/'.format(bucket, job_name_prefix),\n",
    "                    'S3DataDistributionType': 'FullyReplicated',\n",
    "                }\n",
    "            },\n",
    "            'RecordWrapperType': 'None',\n",
    "        }\n",
    "    ],\n",
    "    'OutputDataConfig': {\n",
    "        'S3OutputPath': 's3://{}/{}/output'.format(bucket, job_name_prefix),\n",
    "    },\n",
    "    'ResourceConfig': {\n",
    "        'InstanceCount': 1,\n",
    "        'InstanceType': 'ml.c4.2xlarge',\n",
    "        'VolumeSizeInGB': 50,\n",
    "    },\n",
    "    'RoleArn': role,\n",
    "    'StoppingCondition': {\n",
    "        'MaxRuntimeInSeconds': 60 * 60,\n",
    "    },\n",
    "    'TrainingJobName': job_name,\n",
    "}\n",
    "\n",
    "\n",
    "print('Training job name: {}'.format(job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above configuration create a SageMaker client and use the client to create a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Amazon SageMaker training job\n",
    "sagemaker.create_training_job(**training_params)\n",
    "\n",
    "# confirm that the training job has started, wait for the job to\n",
    "# finish, and then report the ending status\n",
    "#\n",
    "# if the job failed, report why\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n",
    "\n",
    "sagemaker.get_waiter('TrainingJob_Created').wait(TrainingJobName=job_name)\n",
    "training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "status = training_info['TrainingJobStatus']\n",
    "print('Training job ended with status: {}'.format(status))\n",
    "\n",
    "if status == 'Failed':\n",
    "    message = sagemaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))\n",
    "    raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message,\n",
    "\n",
    "> `Training job ended with status: Completed`\n",
    "\n",
    "then that means training sucessfully completed and the output LDA model was stored in the output path specified by `training_params['OutputDataConfig']`.\n",
    "\n",
    "You can also view information about and the status of a training job using the AWS SageMaker console. Just click on the \"Jobs\" tab and select training job matching the training job name, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training job name: {}'.format(job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "***\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model we computed to perform inference on data. For this example, that means predicting the topic mixture representing a given document.\n",
    "\n",
    "This section involves several steps,\n",
    "\n",
    "1. [Create Model](#CreateModel) - Create a hosting model from the trainng output.\n",
    "1. [Create Endpoint Configuration](#CreateEndpointConfiguration) - Create a configuration defining an endpoint.\n",
    "1. [Create Endpoint](#CreateEndpoint) - Use the configuration to create an inference endpoint.\n",
    "1. [Perform Inference](#Perform Inference) - Perform inference on some input data using the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "We now use the output from our training job to create a formal LDA model. This model will serve as the core of our inference hosting endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = job_name\n",
    "model_data_url = sagemaker.describe_training_job(TrainingJobName=job_name)['ModelArtifacts']['S3ModelArtifacts']\n",
    "print('Model data located at {}'.format(model_data_url))\n",
    "\n",
    "model_response = sagemaker.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': model_data_url,\n",
    "    }\n",
    ")\n",
    "print('\\nModel ARN: {}'.format(model_response['ModelArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Endpoint Configuration\n",
    "\n",
    "Next, we use the model to create an endpoint configuration. The endpoint configuration also contains information about the type and number of EC2 instances to use when hosting the algorithm for inference.\n",
    "\n",
    "Recommended instance type: `ml.c4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = job_name_prefix + '-endpoint-config' + timestamp\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "\n",
    "endpoint_config_response = sagemaker.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.c4.2xlarge',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName': 'AllTraffic',\n",
    "    }]\n",
    ")\n",
    "print('\\nEndpoint configuration ARN:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Endpoint\n",
    "\n",
    "Finally, we use the endpoint configuration to create a SageMaker LDA hosting enpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = job_name_prefix + '-endpoint' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_response = sagemaker.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "print('\\nEndpoint ARN = {}'.format(endpoint_response['EndpointArn']))\n",
    "\n",
    "\n",
    "# confirm that endpoint creation has started, wait for the creation\n",
    "# process to finish, and then report the ending status\n",
    "response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('\\nEndpoint creation current status: {}'.format(status))\n",
    "\n",
    "sagemaker.get_waiter('Endpoint_Created').wait(EndpointName=endpoint_name)\n",
    "endpoint_response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with status: {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If you see the message,\n",
    "\n",
    "> `Endpoint creation ended with status = InService`\n",
    "\n",
    "then congratulations! You now have a functioning SageMaker LDA inference endpoint. You can confirm the endpoint configuration and status by navigating to the \"Endpoints\" tab in the AWS SageMaker console and selecting the endpoint matching the endpoint name, below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Endpoint name: {}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference\n",
    "\n",
    "With this realtime endpoint at our fingertips we can finally perform inference on our training and test data.\n",
    "\n",
    "We can pass a variety of data formats to our inference endpoint. In this example we will demonstrate passing CSV-formatted data. Other available formats are JSON-formatted, JSON-sparse-formatter, and RecordIO Protobuf.\n",
    "\n",
    "Below is a helper function which will convert Numpy arrays to CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np2csv(arr):\n",
    "    csv = io.BytesIO()\n",
    "    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n",
    "    return csv.getvalue().decode().rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert some test document data to CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = np2csv(documents_test[:12])\n",
    "print('CSV payload:\\n\\n{}'.format(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we connect to the SageMaker LDA Endpoint and invoke it using this payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_lda_runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "print('Invoking endpoint...')\n",
    "response = sagemaker_lda_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='text/csv',\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "print('\\nResponse:')\n",
    "results = json.loads(response['Body'].read().decode())\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be hard to see but the output format of SageMaker LDA inference endpoint is a Python dictionary with the following format.\n",
    "\n",
    "```\n",
    "{\n",
    "  'predictions': [\n",
    "    {'topic_mixture': [ ... ] },\n",
    "    {'topic_mixture': [ ... ] },\n",
    "    {'topic_mixture': [ ... ] },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "We extract the topic mixtures, themselves, corresponding to each of the input documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_topic_mixtures = np.array([prediction['topic_mixture'] for prediction in results['predictions']])\n",
    "\n",
    "print(computed_topic_mixtures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you decide to compare these results to the known topic mixtures generated in [Obtain Example Data](#ObtainExampleData) keep in mind that SageMaker LDA discovers topics in no particular order. That is, the approximate topic mixtures computed above may be permutations of the known topic mixtures corresponding to the same documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_mixtures_test[0])      # known test topic mixture\n",
    "print(computed_topic_mixtures[0])  # computed topic mixture (topics permuted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop / Close the Endpoint\n",
    "\n",
    "Finally, we should delete the endpoint before we close the notebook.\n",
    "\n",
    "To do so uncomment and execute the cell below. Alternately, you can navigate to the \"Endpoints\" tab in the SageMaker console, select the endpoint with the name stored in the variable `endpoint_name`, and select \"Delete\" from the \"Actions\" dropdown menu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epilogue\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we,\n",
    "\n",
    "* generated some example LDA documents and their corresponding topic-mixtures,\n",
    "* trained a SageMaker LDA model on a training set of documents,\n",
    "* created an inference endpoint,\n",
    "* used the endpoint to infer the topic mixtures of a test input.\n",
    "\n",
    "There are several things to keep in mind when applying SageMaker LDA to real-word data such as a corpus of text documents. Note that input documents to the algorithm, both in training and inference, need to be vectors of integers representing word counts. Each index corresponds to a word in the corpus vocabulary. Therefore, one will need to \"tokenize\" their corpus vocabulary.\n",
    "\n",
    "$$\n",
    "\\text{\"cat\"} \\mapsto 0, \\; \\text{\"dog\"} \\mapsto 1 \\; \\text{\"bird\"} \\mapsto 2, \\ldots\n",
    "$$\n",
    "\n",
    "Each text document then needs to be converted to a \"bag-of-words\" format document.\n",
    "\n",
    "$$\n",
    "w = \\text{\"cat bird bird bird cat\"} \\quad \\longmapsto \\quad w = [2, 0, 3, 0, \\ldots, 0]\n",
    "$$\n",
    "\n",
    "Also note that many real-word applications have large vocabulary sizes. It may be necessary to represent the input documents in sparse format. Finally, stemming and lemmatization can improve compute time, by reducing the effective vocabulary size, but also topic quality of the learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
